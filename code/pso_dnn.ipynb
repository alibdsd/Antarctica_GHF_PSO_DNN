{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pyswarm import pso  # Ensure 'pyswarm' is installed\n",
    "import os\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from pyswarm import pso  # Ensure 'pyswarm' is installed\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pyswarm import pso  # Ensure you have 'pyswarm' installed for PSO\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    " # 确保这个文件包含 get_model 函数和必要的模型结构\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import os\n",
    "import csv\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_model(model, test_loader, criterion,device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            predictions.extend(outputs.view(-1).cpu().numpy())\n",
    "            actuals.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    nrmse = rmse / np.mean(actuals)\n",
    "\n",
    "    print(f'Test Loss: {avg_loss:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'R2: {r2:.4f}')\n",
    "    print(f'NRMSE: {nrmse:.4f}')\n",
    "    return avg_loss, mae, rmse, r2, nrmse, actuals, predictions\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# 修改 CustomModel 类，添加 batch_size 检查\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, hidden_layers, neurons_per_layer, input_shape, output_shape=1):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_shape, neurons_per_layer))\n",
    "        layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(neurons_per_layer, output_shape))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 如果 batch_size=1，跳过 BatchNorm\n",
    "        if x.size(0) == 1:\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(self.input_shape, self.neurons_per_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "            for _ in range(self.hidden_layers):\n",
    "                layers.append(nn.Linear(self.neurons_per_layer, self.neurons_per_layer))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(self.neurons_per_layer, 1))\n",
    "            temp_model = nn.Sequential(*layers)\n",
    "            return temp_model(x)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Function to create the model with given parameters\n",
    "def get_model(hidden_layers, neurons_per_layer, input_shape, output_shape=1):\n",
    "    \"\"\"\n",
    "    根据隐藏层数、每层神经元数量和输入输出形状创建模型实例。\n",
    "\n",
    "    参数:\n",
    "    - hidden_layers: int，隐藏层的数量\n",
    "    - neurons_per_layer: int，每个隐藏层中的神经元数量\n",
    "    - input_shape: int，输入特征的数量\n",
    "    - output_shape: int，模型的输出大小，默认为1\n",
    "\n",
    "    返回:\n",
    "    - model: CustomModel实例\n",
    "    \"\"\"\n",
    "    model = CustomModel(hidden_layers=hidden_layers,\n",
    "                        neurons_per_layer=neurons_per_layer,\n",
    "                        input_shape=input_shape,\n",
    "                        output_shape=output_shape)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, device, model_save_path, fig_name, num_epochs=200, l2_lambda=0.01, pso_params=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    min_val_loss = float('inf')\n",
    "    output_dir = 'pso_models_and_figs'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss += l2_lambda * l2_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * inputs.size(0)\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "                loss += l2_lambda * l2_norm\n",
    "                total_val_loss += loss.item() * inputs.size(0)\n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # 保存最佳模型并显示PSO搜索的超参数\n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'Saved new best model at epoch {epoch+1} with Val Loss: {avg_val_loss:.4f}')\n",
    "            print(f'Current PSO parameters: {pso_params}')\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        early_stopping(avg_val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f'train_val_loss_{fig_name}.png'))\n",
    "\n",
    "# 修改 pso_objective 函数，确保 batch_size 不会导致问题\n",
    "def pso_objective(params):\n",
    "    batch_size = max(int(params[0]), 2)  # 确保 batch_size 至少为2\n",
    "    hidden_layers = int(params[1])\n",
    "    neurons_per_layer = int(params[2])\n",
    "\n",
    "    model = get_model(hidden_layers=hidden_layers, \n",
    "                     input_shape=X_train_sub.shape[1], \n",
    "                     neurons_per_layer=neurons_per_layer)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.004)\n",
    "    \n",
    "    # 在 DataLoader 中使用 drop_last=True，确保最后一个不足的batch被丢弃\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                            shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "\n",
    "    model_save_path = f\"pso_best_model_fold{fold}_{batch_size}_{hidden_layers}_{neurons_per_layer}.pth\"\n",
    "    fig_name = f\"fold{fold}_loss_fig_{batch_size}_{hidden_layers}_{neurons_per_layer}\"\n",
    "    \n",
    "    try:\n",
    "        train_and_validate(model, train_loader, val_loader, criterion, optimizer, device, \n",
    "                         model_save_path, fig_name, num_epochs=200, pso_params=params)\n",
    "        \n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        val_loss, *_ = evaluate_model(model, val_loader, criterion, device)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error in training with params {params}: {str(e)}\")\n",
    "        return float('inf')  # 返回一个大值，让PSO避免这个参数组合\n",
    "        \n",
    "    return val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (保持前面的导入和类定义不变直到数据处理部分) ...\n",
    "\n",
    "import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "# 数据处理部分\n",
    "data = pd.read_csv('D:\\Antarcatica_GHF\\cor_ghf_subsets\\World_train_data.csv')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "categorical_cols = ['Rocktype', 'TectReign']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "X = data.drop(columns=['lon', 'lat', 'heat flow'])\n",
    "y = data['heat flow']\n",
    "\n",
    "# 保存 label_encoders\n",
    "joblib.dump(label_encoders, 'label_encoders.joblib')\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 转换为 torch 张量\n",
    "X_tensor = torch.tensor(X_scaled).float().to(device)  # 确保初始张量在正确设备上\n",
    "y_tensor = torch.tensor(y.values).float().view(-1, 1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# 定义五折交叉验证\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "all_fold_params = []\n",
    "all_fold_metrics = []\n",
    "all_fold_predictions = []  # 存储每折的预测结果\n",
    "# PSO 搜索超参数\n",
    "lb = [64, 4, 32]  # batch_size, hidden_layers, neurons_per_layer 下界\n",
    "ub = [128, 7, 128]  # 上界\n",
    "\n",
    "# 存储每个折的最佳参数和结果\n",
    "all_fold_params = []\n",
    "all_fold_metrics = []\n",
    "# 用于估计不确定性的多次预测次数\n",
    "NUM_SAMPLES = 3  # 可以调整此值，越多越精确但计算时间更长\n",
    "# 修改交叉验证循环中的数据加载部分\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_scaled)):\n",
    "    print(f\"\\nFold {fold + 1}/5\")\n",
    "    \n",
    "    X_train_fold = X_tensor[train_idx].to(device)\n",
    "    y_train_fold = y_tensor[train_idx].to(device)\n",
    "    X_test_fold = X_tensor[test_idx].to(device)\n",
    "    y_test_fold = y_tensor[test_idx].to(device)\n",
    "    \n",
    "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
    "        X_train_fold, y_train_fold, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train_sub = torch.tensor(X_train_sub).float().to(device)\n",
    "    y_train_sub = torch.tensor(y_train_sub).float().to(device)\n",
    "    X_val_sub = torch.tensor(X_val_sub).float().to(device)\n",
    "    y_val_sub = torch.tensor(y_val_sub).float().to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_sub, y_train_sub)\n",
    "    val_dataset = TensorDataset(X_val_sub, y_val_sub)\n",
    "    test_dataset = TensorDataset(X_test_fold, y_test_fold)\n",
    "    \n",
    "    # 确保数据集足够大\n",
    "    if len(train_dataset) < 2 or len(val_dataset) < 2:\n",
    "        print(f\"Warning: Fold {fold + 1} has insufficient data. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # 修改 pso_objective 函数以使用当前的fold数据\n",
    "    def pso_objective(params):\n",
    "        batch_size = max(int(params[0]), 2)\n",
    "        hidden_layers = int(params[1])\n",
    "        neurons_per_layer = int(params[2])\n",
    "\n",
    "        model = get_model(hidden_layers=hidden_layers, \n",
    "                         input_shape=X_train_sub.shape[1], \n",
    "                         neurons_per_layer=neurons_per_layer).to(device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.004)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                               shuffle=False, drop_last=True)\n",
    "\n",
    "        model_save_path = f\"pso_best_model_fold{fold}_{batch_size}_{hidden_layers}_{neurons_per_layer}.pth\"\n",
    "        fig_name = f\"fold{fold}_loss_fig_{batch_size}_{hidden_layers}_{neurons_per_layer}\"\n",
    "        \n",
    "        try:\n",
    "            train_and_validate(model, train_loader, val_loader, criterion, optimizer, device, \n",
    "                             model_save_path, fig_name, num_epochs=200, pso_params=params)\n",
    "            \n",
    "            model.load_state_dict(torch.load(model_save_path))\n",
    "            val_loss, *_ = evaluate_model(model, val_loader, criterion, device)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in training with params {params}: {str(e)}\")\n",
    "            return float('inf')\n",
    "        \n",
    "        return val_loss\n",
    "\n",
    "    # 执行 PSO 优化\n",
    "    best_params, best_val_loss = pso(pso_objective, lb, ub, swarmsize=10, maxiter=10)\n",
    "    \n",
    "    # 保存最佳参数\n",
    "    best_batch_size = int(best_params[0])\n",
    "    best_hidden_layers = int(best_params[1])\n",
    "    best_neurons_per_layer = int(best_params[2])\n",
    "    \n",
    "    print(f'Fold {fold + 1} - Optimal Parameters: Batch Size: {best_batch_size}, '\n",
    "          f'Hidden Layers: {best_hidden_layers}, Neurons/Layer: {best_neurons_per_layer}')\n",
    "    print(f'Fold {fold + 1} - Best Validation Loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    # 使用最佳参数训练最终模型并评估测试集\n",
    "    best_model = get_model(best_hidden_layers, best_neurons_per_layer, \n",
    "                          input_shape=X_train_sub.shape[1]).to(device)\n",
    "    best_model_save_path = f'pso_best_model_fold{fold}_{best_batch_size}_{best_hidden_layers}_{best_neurons_per_layer}.pth'\n",
    "    best_model.load_state_dict(torch.load(best_model_save_path))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "    test_loss, mae, rmse, r2, nrmse, actuals, predictions = evaluate_model(\n",
    "        best_model, test_loader, torch.nn.MSELoss(), device\n",
    "    )\n",
    "    # 计算预测值和标准差（通过多次预测）\n",
    "    best_model.eval()\n",
    "    all_fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(NUM_SAMPLES):\n",
    "            fold_preds = []\n",
    "            for inputs, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = best_model(inputs)\n",
    "                fold_preds.extend(outputs.view(-1).cpu().numpy())\n",
    "            all_fold_preds.append(fold_preds)\n",
    "\n",
    "    # 计算每折的预测均值和标准差\n",
    "    all_fold_preds = np.array(all_fold_preds)  # (NUM_SAMPLES, n_test_samples)\n",
    "    fold_mean_preds = np.mean(all_fold_preds, axis=0)\n",
    "    fold_std_preds = np.std(all_fold_preds, axis=0)\n",
    "    \n",
    "    # 保存每折的预测结果到CSV，包括预测值和标准差\n",
    "    fold_df = pd.DataFrame({\n",
    "        'lon': data.iloc[test_idx]['lon'],\n",
    "        'lat': data.iloc[test_idx]['lat'],\n",
    "        'actual_heat_flow': actuals,\n",
    "        'predicted_heat_flow': fold_mean_preds,\n",
    "        'predicted_std': fold_std_preds\n",
    "    })\n",
    "\n",
    "    fold_csv_path = f'fold_{fold + 1}_predictions_with_std.csv'\n",
    "    fold_df.to_csv(fold_csv_path, index=False)\n",
    "    print(f\"Saved Fold {fold + 1} predictions with std to {fold_csv_path}\")\n",
    "\n",
    "    \n",
    "    all_fold_predictions.append((fold_mean_preds, fold_std_preds, test_idx))\n",
    "    all_fold_params.append([best_batch_size, best_hidden_layers, best_neurons_per_layer])\n",
    "    all_fold_metrics.append({'test_loss': test_loss, 'mae': mae, 'rmse': rmse, 'r2': r2, 'nrmse': nrmse})\n",
    "\n",
    "\n",
    "\n",
    "# 集成五折预测结果\n",
    "n_samples = X_scaled.shape[0]\n",
    "ensemble_preds = np.zeros((5, n_samples))  # 存储每折的预测均值\n",
    "ensemble_stds = np.zeros((5, n_samples))   # 存储每折的预测标准差\n",
    "for fold, (fold_preds, fold_stds, test_idx) in enumerate(all_fold_predictions):\n",
    "    ensemble_preds[fold, test_idx] = fold_preds\n",
    "    ensemble_stds[fold, test_idx] = fold_stds\n",
    "\n",
    "# 计算集成均值和标准差\n",
    "ensemble_mean = np.zeros(n_samples)\n",
    "ensemble_std = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    valid_preds = ensemble_preds[:, i][ensemble_preds[:, i] != 0]  # 排除未预测的（0值）\n",
    "    valid_stds = ensemble_stds[:, i][ensemble_stds[:, i] != 0]\n",
    "    if len(valid_preds) > 0:\n",
    "        ensemble_mean[i] = np.mean(valid_preds)\n",
    "        # 集成标准差：结合模型间方差和模型内方差\n",
    "        ensemble_std[i] = np.sqrt(np.mean(valid_stds**2) + np.var(valid_preds))\n",
    "\n",
    "# 保存集成结果\n",
    "ensemble_df = pd.DataFrame({\n",
    "    'lon': data['lon'],\n",
    "    'lat': data['lat'],\n",
    "    'actual_heat_flow': y,\n",
    "    'ensemble_predicted_heat_flow': ensemble_mean,\n",
    "    'ensemble_predicted_std': ensemble_std\n",
    "})\n",
    "ensemble_df.to_csv('ensemble_predictions_with_std.csv', index=False)\n",
    "print(\"Saved ensemble predictions with std to 'ensemble_predictions_with_std.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 集成测试\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "# 定义模型类（与训练时相同）\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, hidden_layers, neurons_per_layer, input_shape, output_shape=1):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_shape, neurons_per_layer))\n",
    "        layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(neurons_per_layer, output_shape))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.size(0) == 1:  # 如果 batch_size=1，跳过 BatchNorm\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(self.input_shape, self.neurons_per_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "            for _ in range(self.hidden_layers):\n",
    "                layers.append(nn.Linear(self.neurons_per_layer, self.neurons_per_layer))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(self.neurons_per_layer, 1))\n",
    "            temp_model = nn.Sequential(*layers)\n",
    "            return temp_model(x)\n",
    "        return self.model(x)\n",
    "\n",
    "def get_model(hidden_layers, neurons_per_layer, input_shape, output_shape=1):\n",
    "    return CustomModel(hidden_layers, neurons_per_layer, input_shape, output_shape)\n",
    "\n",
    "# 假设推理数据集路径为 'Antar_test_data.csv'\n",
    "inference_data = pd.read_csv('Antar_test_data_with_heatflux.csv')\n",
    "\n",
    "# 加载保存的 LabelEncoder 和 Scaler\n",
    "label_encoders = joblib.load('label_encoders.joblib')\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "categorical_cols = ['Rocktype', 'TectReign']\n",
    "for col in categorical_cols:\n",
    "    try:\n",
    "        inference_data[col] = label_encoders[col].transform(inference_data[col])\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Unknown categories in {col}. Filling with default value.\")\n",
    "        inference_data[col] = inference_data[col].apply(\n",
    "            lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1\n",
    "        )\n",
    "\n",
    "# 提取特征\n",
    "X_new = inference_data.drop(columns=['lon', 'lat', 'heat flow'])\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# 转换为张量\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_new_tensor = torch.tensor(X_new_scaled).float().to(device)\n",
    "\n",
    "# 五折模型的参数（根据你的训练结果更新）\n",
    "fold_params = [\n",
    "    (107, 7, 123),  # Fold 1: Batch Size: 107, Hidden Layers: 7, Neurons/Layer: 123\n",
    "    (78, 6, 102),   # Fold 2: Batch Size: 78, Hidden Layers: 6, Neurons/Layer: 102\n",
    "    (90, 5, 97),    # Fold 3: Batch Size: 90, Hidden Layers: 5, Neurons/Layer: 97\n",
    "    (126, 6, 79),   # Fold 4: Batch Size: 126, Hidden Layers: 6, Neurons/Layer: 79\n",
    "    (128, 6, 103),  # Fold 5: Batch Size: 128, Hidden Layers: 6, Neurons/Layer: 103\n",
    "]\n",
    "\n",
    "# 创建保存预测的目录\n",
    "output_dir = 'fold_predictions'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 集成预测并保存每折结果\n",
    "all_predictions = []\n",
    "for fold, (batch_size, hidden_layers, neurons) in enumerate(fold_params):\n",
    "    print(f\"Loading and predicting with model for Fold {fold + 1}\")\n",
    "    model = get_model(hidden_layers, neurons, X_new_tensor.shape[1]).to(device)\n",
    "    model_path = f'pso_best_model_fold{fold}_{batch_size}_{hidden_layers}_{neurons}.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    loader = DataLoader(TensorDataset(X_new_tensor, torch.zeros_like(X_new_tensor[:, :1])), \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            fold_predictions.extend(outputs.view(-1).cpu().numpy())\n",
    "    all_predictions.append(fold_predictions)\n",
    "    \n",
    "    # 保存每折的预测结果到 CSV\n",
    "    fold_df = inference_data[['lon', 'lat']].copy()\n",
    "    fold_df['predicted_heat_flow'] = fold_predictions\n",
    "    fold_csv_path = os.path.join(output_dir, f'fold_{fold + 1}_inference_predictions.csv')\n",
    "    fold_df.to_csv(fold_csv_path, index=False)\n",
    "    print(f\"Saved Fold {fold + 1} predictions to {fold_csv_path}\")\n",
    "\n",
    "# 计算集成预测的均值和标准差\n",
    "predictions = np.mean(all_predictions, axis=0)\n",
    "y_std = np.std(all_predictions, axis=0)\n",
    "\n",
    "# 将预测均值和标准差添加到数据框\n",
    "inference_data['ensemble_predicted_heat_flow'] = predictions\n",
    "inference_data['ensemble_predicted_std'] = y_std\n",
    "\n",
    "# 评估有真实值的行\n",
    "if 'heat flow' in inference_data.columns:\n",
    "    valid_indices = inference_data['heat flow'].notna()\n",
    "    y_true = inference_data.loc[valid_indices, 'heat flow'].values\n",
    "    y_pred = inference_data.loc[valid_indices, 'ensemble_predicted_heat_flow'].values\n",
    "    \n",
    "    if len(y_true) > 0:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f\"\\nPerformance on {len(y_true)} rows with true values (Ensemble):\")\n",
    "        print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid true values to evaluate.\")\n",
    "\n",
    "# 保存集成结果\n",
    "output_file = 'ensemble_inference_predictions_with_std.csv'\n",
    "inference_data.to_csv(output_file, index=False)\n",
    "print(f\"Ensemble predictions saved to {output_file}\")\n",
    "\n",
    "# 显示前几行结果\n",
    "print(\"\\nSample of the updated dataset:\")\n",
    "print(inference_data[['lon', 'lat', 'heat flow', 'ensemble_predicted_heat_flow', 'ensemble_predicted_std']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Import Uncertainty Toolbox\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "# Set random seed (optional, since we’re using real data)\n",
    "seed = 111\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set plot style (as in the tutorial)\n",
    "uct.viz.set_style()\n",
    "uct.viz.update_rc('figure.dpi', 130)\n",
    "uct.viz.update_rc('text.usetex', False)\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('ensemble_inference_predictions_with_std4.csv')  # Replace with your CSV file path\n",
    "\n",
    "# Extract data\n",
    "y_true = df['heat flow'].values  # Observed values (equivalent to te_y in the tutorial)\n",
    "pred_mean = df['ensemble_predicted_heat_flow'].values  # Predicted means (equivalent to pred_mean)\n",
    "pred_std = df['ensemble_predicted_std'].values  # Predicted standard deviations (equivalent to pred_std)\n",
    "\n",
    "# Ensure data is clean (no NaN or infinite values)\n",
    "mask = ~np.isnan(y_true) & ~np.isnan(pred_mean) & ~np.isnan(pred_std)\n",
    "y_true = y_true[mask]\n",
    "pred_mean = pred_mean[mask]\n",
    "pred_std = pred_std[mask]\n",
    "\n",
    "# Modify x-axis for the first plot: Sort data by predicted_heat_flow to order from smallest to largest\n",
    "sort_idx = np.argsort(pred_mean)  # Sort by predicted_heat_flow\n",
    "x_sorted = np.arange(len(pred_mean))[sort_idx]  # Create an index (0 to n-1) for the sorted order\n",
    "y_true_sorted = y_true[sort_idx]\n",
    "pred_mean_sorted = pred_mean[sort_idx]\n",
    "pred_std_sorted = pred_std[sort_idx]\n",
    "\n",
    "# 1. Create a figure with three subplots in a row\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))  # 1 row, 3 columns, adjusted figure size\n",
    "\n",
    "# 1. Plot Confidence Bands on Test Data (using sorted order instead of lon)\n",
    "uct.viz.plot_xy(pred_mean_sorted, pred_std_sorted, y_true_sorted, x_sorted, ax=ax1)  # Use sorted index as x-axis\n",
    "ax1.set_title('Confidence Bands')\n",
    "ax1.set_xlabel('Index (Ordered by Observed Heat Flux)')\n",
    "ax1.set_ylabel('Heat Flux')\n",
    "\n",
    "# 2. Plot Ordered Prediction Intervals\n",
    "uct.viz.plot_intervals_ordered(pred_mean, pred_std, y_true, ax=ax2)\n",
    "ax2.set_title('Ordered Prediction Intervals')\n",
    "\n",
    "# 3. Plot Average Calibration\n",
    "uct.viz.plot_calibration(pred_mean, pred_std, y_true, ax=ax3)\n",
    "ax3.set_title('Average Calibration')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Plot Adversarial Group Calibration (optional, as in the tutorial)\n",
    "plt.figure(figsize=(5, 3))\n",
    "uct.viz.plot_adversarial_group_calibration(pred_mean, pred_std, y_true)\n",
    "plt.title('Adversarial Group Calibration')\n",
    "plt.show()\n",
    "\n",
    "# 5. Get All Metrics (optional, for evaluation)\n",
    "pnn_metrics = uct.metrics.get_all_metrics(pred_mean, pred_std, y_true)\n",
    "print(\"Uncertainty Metrics:\", pnn_metrics)\n",
    "\n",
    "# Optional: Recalibrate Predictive Uncertainties (as in the tutorial, if you want to improve calibration)\n",
    "# This requires a separate recalibration dataset, which you’d need to generate or provide.\n",
    "# For simplicity, we’ll skip this step unless you provide additional data for recalibration.\n",
    "\n",
    "# Optional: Visualize 95% Prediction Intervals (you can adapt this from the tutorial)\n",
    "# Note: We’ll use the original unsorted x (lon) here, but you can modify it similarly if needed\n",
    "orig_bounds = uct.metrics_calibration.get_prediction_interval(pred_mean, pred_std, 0.95, None)\n",
    "x = df['lon'].values[mask]  # Reusing lon as x for consistency with the original plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.fill_between(x, orig_bounds.lower, orig_bounds.upper, alpha=0.6, label='Original 95% Interval')\n",
    "plt.scatter(x, y_true, color='black', alpha=0.5, label='Observed Heat Flux')\n",
    "plt.xlabel('Longitude (lon)')\n",
    "plt.ylabel('Heat Flux')\n",
    "plt.title('95% Prediction Interval (Spatial)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Recalibrate by scaling standard deviations (as in the tutorial)\n",
    "# This would require a separate recalibration dataset, which you’d need to provide.\n",
    "# For now, we’ll skip this unless you have additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
